# Machine Learning Books

The following is a list of machine learning books that are introductory, comprehensive or uniquely authoritative in their subject matter (e.g. a foundational book on kernels). Note that many of these texts are available online for free by the authors.

- [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage). David Barber. Cambridge University Press, 2012.
> This hands-on text on machine learning is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.
- [Machine Learning: A Probabilistic Perspective](http://www.cs.ubc.ca/~murphyk/MLbook/). Kevin P. Murphy. MIT Press, 2012.
> According to the author, the book is similar to Bishop's "Pattern Recognition and Machine Learning", Hastie et al's "The Elements of Statistical Learning", and to Wasserman's "All of Statistics", with the following key differences:
> + It is more accessible to undergrads. It pre-supposes a background in probability, linear algebra, calculus, and programming; however, the mathematical level ramps up slowly, with more difficult sections clearly denoted as such. This makes the book suitable for both undergrads and grads. Summaries of the relevant mathematical background, on topics such as linear algebra, optimization and classical statistics make the book self-contained.
> + It is more practically-oriented. In particular, it comes with Matlab software to reproduce almost every figure, and to implement almost every algorithm, discussed in the book. It includes many worked examples of the methods applied to real data, with readable source code online.
> + It covers various important topics that are not discussed in these other books, such as conditional random fields, deep learning, etc.
> + It is "more Bayesian" than the Hastie or Wasserman books, but "more frequentist" than the Bishop book. In particular we make extensive use of MAP estimation, which we regard as "poor man's Bayes". We prefer this to the regularization interpretation of MAP, because then all the methods in the book (except cross validation...) can be viewed as probabilistic inference, or some approximation thereof. The MAP interpretation also allows for an easy "upgrade path" to more accurate methods of approximate Bayesian inference, such as empirical Bayes, variational Bayes, MCMC, SMC, etc.
> + The emphasis is on simple parametric models (linear and logistic regression, discriminant analysis/naive Bayes, mixture models, factor analysis, graphical models, etc.), which are the ones most often used in practice. However, we also briefly discuss non-parametric models, such as Gaussian processes, Dirichlet processes, SVMs, RVMs, etc. 
- [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/). Christopher M. Bishop. Springer, 2006.
> This leading textbook provides a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. This is the first machine learning textbook to include a comprehensive coverage of recent developments such as probabilistic graphical models and deterministic inference methods, and to emphasize a modern Bayesian perspective.
- [The Elements of Statistical Learning: Data Mining, Inference and Prediction](http://statweb.stanford.edu/~tibs/ElemStatLearn/). Trevor Hastie, Robert Tibshirani and Jerome Friedman. Springer, 2009.
- [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/). Stuart Russel, Peter Norvig. Pearson, 2009.
- [Learning with Kernels](http://agbs.kyb.tuebingen.mpg.de/lwk/). Bernhard SchÃ¶lkopf and Alex Smola. MIT Press, 2002.
- [Mining of Massive Datasets](http://www.mmds.org). Jure Leskovec, Anand Rajaraman and Jeffrey D. Ullman. Cambridge University Press, 2011.
> In contrast to other references in this area, this book focuses on the implementation and practical use of machine learning algorithms that have been used to solve key problems in data mining and which can be used on even the largest datasets. It begins with a discussion of the map-reduce framework, an important tool for parallelizing algorithms. The authors explain the tricks of locality-sensitive hashing and stream processing algorithms for mining data that arrives too fast for exhaustive processing. The PageRank idea and related tricks for organizing the Web are covered next. Other chapters cover the problems of finding frequent itemsets and clustering. The final chapters cover two applications: recommendation systems and Web advertising, each vital in e-commerce. Written by two authorities in database and Web technologies, this book is essential reading for students and practitioners alike.
